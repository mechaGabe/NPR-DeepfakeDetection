# Attention-NPR Architecture Diagram
## Detailed Visual Specifications for Presentation

---

## MAIN ARCHITECTURE DIAGRAM

### Layout: Left-to-Right Flow

```
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ                    ATTENTION-WEIGHTED MULTI-SCALE NPR                   â”ƒ
â”—â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”›

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              â”‚
â”‚   Input      â”‚
â”‚   Image      â”‚        STAGE 1: Multi-Scale NPR Extraction
â”‚ (3Ã—224Ã—224)  â”‚        â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â”‚              â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚    â”‚                                                â”‚
       â”œâ”€â”€â”€â”€â”¤  Scale 0.25x: Downsampleâ†’Upsampleâ†’Residual   â”‚â”€â”€â†’ NPR_0.25
       â”‚    â”‚  F.interpolate(x, 0.25) â†’ (x - x_recon)      â”‚   (3Ã—224Ã—224)
       â”‚    â”‚                                                â”‚
       â”œâ”€â”€â”€â”€â”¤  Scale 0.50x: Downsampleâ†’Upsampleâ†’Residual   â”‚â”€â”€â†’ NPR_0.50
       â”‚    â”‚  F.interpolate(x, 0.50) â†’ (x - x_recon)      â”‚   (3Ã—224Ã—224)
       â”‚    â”‚  [ORIGINAL NPR SCALE]                         â”‚
       â”‚    â”‚                                                â”‚
       â”œâ”€â”€â”€â”€â”¤  Scale 0.75x: Downsampleâ†’Upsampleâ†’Residual   â”‚â”€â”€â†’ NPR_0.75
       â”‚    â”‚  F.interpolate(x, 0.75) â†’ (x - x_recon)      â”‚   (3Ã—224Ã—224)
       â”‚    â”‚                                                â”‚
       â””â”€â”€â”€â”€â”¤  Scale 1.00x: Identity (No downsampling)      â”‚â”€â”€â†’ NPR_1.00
            â”‚  NPR_1.00 = Original Image                    â”‚   (3Ã—224Ã—224)
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Concatenate Channel-Wiseâ”‚
                    â”‚     (4 scales Ã— 3 RGB)   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                        (12Ã—224Ã—224)
                                 â”‚
                                 â–¼

       STAGE 2: Channel Attention Module (SENet-Inspired)
       â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Global Avg Pool    â”‚
                    â”‚  12Ã—224Ã—224 â†’ 12Ã—1Ã—1â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Squeeze: FC Layer  â”‚
                    â”‚    12 â†’ 6 (Ã·2)      â”‚
                    â”‚    Activation: ReLU â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Excite: FC Layer   â”‚
                    â”‚    6 â†’ 12 (Ã—2)      â”‚
                    â”‚  Activation: Sigmoidâ”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Reshape & Tile    â”‚
                    â”‚   12 â†’ (4, 3, 1, 1) â”‚
                    â”‚   4 scales Ã— 3 ch   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚  Attention Weights      â”‚
                  â”‚  w1, w2, w3, w4         â”‚
                  â”‚  (per RGB channel)      â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼

       STAGE 3: Weighted Fusion
       â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚  Element-wise Multiply:   â”‚
            â”‚  NPR_0.25 Ã— w1            â”‚
            â”‚  NPR_0.50 Ã— w2            â”‚
            â”‚  NPR_0.75 Ã— w3            â”‚
            â”‚  NPR_1.00 Ã— w4            â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚  Sum across scales:       â”‚
            â”‚  NPR_fused = Î£(wi Ã— NPRi) â”‚
            â”‚  Output: (3Ã—224Ã—224)      â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼

       STAGE 4: ResNet50 Classifier
       â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚  Conv1: 3â†’64, 3Ã—3, s=2    â”‚ 112Ã—112
            â”‚  BatchNorm + ReLU         â”‚
            â”‚  MaxPool: 3Ã—3, s=2        â”‚ 56Ã—56
            â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
            â”‚  Layer1: BasicBlockÃ—3     â”‚ 56Ã—56
            â”‚  64 â†’ 256 channels        â”‚
            â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
            â”‚  Layer2: BasicBlockÃ—4     â”‚ 28Ã—28
            â”‚  256 â†’ 512, stride=2      â”‚
            â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
            â”‚  Layer3: BasicBlockÃ—6     â”‚ 14Ã—14
            â”‚  512 â†’ 1024, stride=2     â”‚
            â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
            â”‚  Layer4: BasicBlockÃ—3     â”‚ 7Ã—7
            â”‚  1024 â†’ 2048, stride=2    â”‚
            â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
            â”‚  Global Avg Pool          â”‚ 1Ã—1
            â”‚  2048 â†’ 2048Ã—1Ã—1          â”‚
            â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
            â”‚  Fully Connected          â”‚
            â”‚  2048 â†’ 1 (logit)         â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚  BCEWithLogitsLoss        â”‚
            â”‚  Sigmoid(logit)           â”‚
            â”‚  Output: P(Fake)          â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## DIAGRAM COMPONENTS FOR POWERPOINT

### Component 1: Multi-Scale NPR Extraction (Parallel Paths)

**Visual Style:** 4 parallel horizontal bars with different colors

```
Scale 0.25x: [BLUE]    â”€â”€â†’  NPRâ‚€.â‚‚â‚…
             â†“â†‘ (artifacts at high frequency)

Scale 0.50x: [GREEN]   â”€â”€â†’  NPRâ‚€.â‚…â‚€  â­ BASELINE
             â†“â†‘ (original NPR scale)

Scale 0.75x: [ORANGE]  â”€â”€â†’  NPRâ‚€.â‚‡â‚…
             â†“â†‘ (artifacts at low frequency)

Scale 1.00x: [RED]     â”€â”€â†’  NPRâ‚.â‚€â‚€
             -- (identity, no downsampling)
```

**PowerPoint Instructions:**
1. Use SmartArt "Process" â†’ "Vertical Process"
2. 4 shapes, each labeled with scale
3. Add small image icons showing downsampling operation
4. Color-code each scale (blue â†’ green â†’ orange â†’ red)

---

### Component 2: Attention Module (Squeeze-and-Excitation)

**Visual Style:** Diamond/funnel shape

```
      [12-dim vector]  â† Concatenated NPR features
            â”‚
            â–¼
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚   GAP   â”‚      Global Average Pooling
      â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
           â”‚
      â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
      â”‚FC: 12â†’6 â”‚      Squeeze (compression)
      â”‚  ReLU   â”‚
      â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
           â”‚
      â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
      â”‚FC: 6â†’12 â”‚      Excitation (expansion)
      â”‚ Sigmoid â”‚
      â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
           â”‚
      [w1,w2,w3,w4]    Scale weights (0-1)
```

**PowerPoint Instructions:**
1. Use shapes: Rectangle â†’ Trapezoid (narrowing) â†’ Trapezoid (widening)
2. Label dimensions on the side
3. Add small mathematical notation: Ïƒ(FC(ReLU(FC(GAP(x)))))

---

### Component 3: Weighted Fusion (Matrix Multiplication)

**Visual Style:** Matrix diagram

```
NPR_0.25  Ã—  w1  =  w1Â·NPR_0.25
NPR_0.50  Ã—  w2  =  w2Â·NPR_0.50    â”
NPR_0.75  Ã—  w3  =  w3Â·NPR_0.75    â”œâ”€â”€â†’  Î£  â†’  NPR_fused
NPR_1.00  Ã—  w4  =  w4Â·NPR_1.00    â”˜
```

**PowerPoint Instructions:**
1. Use table with 4 rows
2. Add Ã— and = symbols
3. Final summation symbol (Î£) in large font
4. Arrows showing flow to final output

---

## COMPARISON DIAGRAM: Baseline vs. Attention-NPR

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 BASELINE NPR-ResNet50                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Input â†’ NPR_0.5 (fixed) â†’ ResNet50 â†’ Output
        (single scale)

        âŒ Cannot adapt to different generators
        âŒ Misses artifacts at other scales


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            ATTENTION-NPR-ResNet50 (Proposed)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Input â†’ Multi-Scale NPR â†’ Attention â†’ Fusion â†’ ResNet50 â†’ Output
        (4 scales)        (learned)    (weighted)

        âœ“ Adapts weights to input characteristics
        âœ“ Captures artifacts across frequency spectrum
        âœ“ Generalizes to unseen generators
```

---

## VISUALIZATION: Attention Weights by Generator Type

**Mock Heatmap for Presentation:**

```
Generator Type    â”‚ w1 (0.25x) â”‚ w2 (0.50x) â”‚ w3 (0.75x) â”‚ w4 (1.00x)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ProGAN            â”‚    0.15    â”‚  â¬› 0.55   â”‚    0.20    â”‚    0.10
StyleGAN2         â”‚    0.20    â”‚  â¬› 0.50   â”‚    0.25    â”‚    0.05
DALL-E 3          â”‚  â¬› 0.45   â”‚    0.25    â”‚    0.20    â”‚    0.10
Midjourney v6     â”‚  â¬› 0.40   â”‚    0.30    â”‚    0.20    â”‚    0.10
FLUX              â”‚  â¬› 0.50   â”‚    0.20    â”‚    0.15    â”‚    0.15
Stable Diffusion  â”‚    0.35    â”‚    0.30    â”‚  â¬› 0.25   â”‚    0.10
```

**Observation (Hypothesis):**
- GANs (ProGAN, StyleGAN2): Higher weight on 0.5x (original NPR scale)
- Diffusion (DALL-E, Midjourney, FLUX): Higher weight on 0.25x (finer artifacts)
- Demonstrates learned scale-specific detection strategy

**PowerPoint Instructions:**
1. Create table with conditional formatting
2. Highest weight per row: dark fill
3. Color gradient from white (low) to dark (high)

---

## TIMELINE GANTT CHART

```
Week 1 (Nov 18-24)     [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] Baseline + Implement
â”‚                       â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ Reproduce baseline
â”‚                               â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ Code multi-scale NPR
â”‚                                       â–ˆâ–ˆâ–ˆâ–ˆ Attention module
â”‚
Week 2 (Nov 25-Dec 1)  [â–‘â–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘] Training
â”‚                           â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ Train Attention-NPR
â”‚                                       â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ Ablation studies
â”‚                                               â–ˆâ–ˆâ–ˆâ–ˆ Test seen gens
â”‚
Week 3 (Dec 2-8)       [â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] Testing & Analysis
â”‚                               â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ Collect 2025 data
â”‚                                       â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ Test unseen gens
â”‚                                               â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ Stats + Viz
â”‚
Week 4 (Dec 9)         [â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆ] Documentation
â”‚                                               â–ˆâ–ˆâ–ˆâ–ˆ Final report
â”‚                                                   ğŸ“… DEC 9 DEADLINE
```

**Critical Milestones:**
- âœ“ Nov 17, 5:00 PM: Presentation submitted âœ…
- â³ Nov 24: Implementation complete
- â³ Dec 1: Training + ablation done
- â³ Dec 8: All experiments complete
- ğŸ“… Dec 9, 10:00 AM: **FINAL SUBMISSION**

---

## HARDWARE REQUIREMENTS DIAGRAM

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    COMPUTE RESOURCES                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

GPU: NVIDIA RTX 3090 / A100
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘       â”‚  24GB VRAM
     â”‚  18GB used (Attention-NPR)       â”‚
     â”‚  Peak: 22GB during training      â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

CPU: 32-core (Intel Xeon / AMD EPYC)
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  â”‚  128GB RAM
     â”‚  24GB used (data loading)        â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Storage: 500GB SSD
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  â”‚  500GB
     â”‚  200GB: Datasets                 â”‚
     â”‚  20GB: Checkpoints               â”‚
     â”‚  10GB: Results/Logs              â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Training Time Estimate:
  Baseline:        18 hours  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘
  Attention-NPR:   24 hours  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
  Ablations (3):   72 hours  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
  Testing:          4 hours  â–ˆâ–ˆâ–ˆ
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  TOTAL:          ~120 hours (5 days continuous)
```

---

## EXPECTED RESULTS VISUALIZATION

### ROC Curve Comparison (Mock)

```
     1.0 â”‚                    â”Œâ”€â”€â”€ Attention-NPR (AUC=0.96)
         â”‚                   â•±â”‚
         â”‚                  â•± â”‚
   TPR   â”‚                 â•±  â”‚
         â”‚                â•±   â””â”€â”€â”€ Baseline (AUC=0.92)
         â”‚               â•±   â•±
         â”‚              â•±   â•±
     0.5 â”‚             â•±   â•±
         â”‚            â•±   â•±
         â”‚           â•±   â•±
         â”‚          â•±   â•±
         â”‚         â•±   â•±
     0.0 â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
         0.0    0.5                1.0
                    FPR
```

### Generalization Gap (Bar Chart)

```
Accuracy (%)
100 â”‚
    â”‚     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ       â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                Baseline: Î”=8%
 95 â”‚     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ       â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                Attention: Î”=3%
    â”‚     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ       â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
 90 â”‚     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ       â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
    â”‚     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ       â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
 85 â”‚     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ               â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
    â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€
         Base-Seen   Attn-Seen  Attn-Unseen

    â¬› Baseline (Seen):    92%
    â¬› Attention (Seen):   95%  (+3% vs baseline)
    â¬› Attention (Unseen): 92%  (only 3% gap)
```

---

## SUCCESS CRITERIA TABLE

| Metric                    | Baseline | Target | Status |
|---------------------------|----------|--------|--------|
| Accuracy (ForenSynths)    | 91.7%    | â‰¥93.7% | â³ TBD |
| Generalization Gap        | ~8%      | <5%    | â³ TBD |
| Attention Interpretability| N/A      | âœ“      | â³ TBD |
| Inference Time Overhead   | 1.0Ã—     | <1.3Ã—  | â³ TBD |
| Memory Overhead           | 12GB     | <24GB  | â³ TBD |

**Statistical Significance:** Paired t-test, p < 0.05

---

## LIMITATIONS SUMMARY (Slide Format)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         KNOWN LIMITATIONS              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. Computational
   â€¢ Limited to 4 scales (memory constraint)
   â€¢ Cannot exhaustively search scale space

2. Data
   â€¢ 2025 generators: limited sample availability
   â€¢ Midjourney v6: no ground truth parameters

3. Architecture
   â€¢ Simple channel attention (SENet-style)
   â€¢ Fixed ResNet50 backbone

4. Evaluation
   â€¢ Post-processing (JPEG, filtering) not fully tested
   â€¢ Real-world deployment not validated
```

---

## SLIDE-BY-SLIDE CONTENT RECOMMENDATIONS

**Slide 1: Title**
- Title: "Attention-Weighted Multi-Scale NPR for Deepfake Detection"
- Subtitle: "Generalizable Detection for 2025 Generators"
- Names, Date, Course

**Slide 2: Motivation**
- Image: Deepfake examples (Midjourney v6, FLUX, DALL-E 3)
- Stat: "91.7% baseline accuracy, but can we do better?"
- Question: "How do we generalize to unseen generators?"

**Slide 3: Background - NPR**
- Diagram: Upsampling operation creating artifacts
- Formula: NPR = x - interpolate(x, scale_factor)
- Current limitation: Fixed 0.5x scale

**Slide 4: Problem Statement**
- "Different generators â†’ Different artifact scales"
- Visual: Frequency analysis showing artifacts at multiple scales
- Gap: "No adaptive scale selection mechanism"

**Slide 5: Our Approach**
- Full architecture diagram (main diagram from above)
- 3 key innovations:
  1. Multi-scale NPR (4 scales)
  2. Learned attention weights
  3. Adaptive fusion

**Slide 6: Hypotheses**
- H2: Scale-adaptive attention improves accuracy (â‰¥2%)
- H3: Better generalization to 2025 generators (gap <5%)
- Visual: Expected attention weight heatmap

**Slide 7: Training Data**
- ForenSynths: 240K images, 4 classes, 8 generators
- Augmentation strategy
- Test sets: Seen (GANGen) + Unseen (FLUX, Midjourney v6, DALL-E 3)

**Slide 8: Timeline**
- Gantt chart (from above)
- Milestones clearly marked
- Dec 9 deadline highlighted

**Slide 9: Hardware & Resources**
- Hardware diagram (from above)
- 120-hour training estimate
- Confirmed availability âœ“

**Slide 10: Success Criteria**
- Table: Baseline vs. Target metrics
- Statistical testing approach
- Minimal acceptable outcome defined

**Slide 11: Expected Results**
- Mock ROC curves
- Generalization gap bar chart
- Attention weight interpretation

**Slide 12: Ablation Studies**
- Fixed equal weights (0.25 each)
- Fixed learned weights (non-adaptive)
- Full adaptive attention
- Comparison table

**Slide 13: Limitations & Risks**
- Summary table (from above)
- Mitigation strategies
- Fallback plans

**Slide 14: Deliverables**
- Code repository (with documentation)
- Trained models (checkpoints)
- Final report (8-10 pages)
- Presentation + supplementary materials

**Slide 15: Questions & Discussion**
- "Which scale do you expect to be most important for FLUX?"
- "Should we explore spatial attention as well?"
- Contact info

---

*This architecture diagram guide provides all visual elements needed for a professional, academic presentation. Each diagram can be created in PowerPoint using built-in SmartArt, shapes, and tables.*
