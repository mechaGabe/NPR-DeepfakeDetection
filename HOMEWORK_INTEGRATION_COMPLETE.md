# Homework Concepts Adapted for Multi-Scale NPR Project

## Complete Homework Analysis & Adaptations

---

## ðŸ“š Homework Overview

| HW | Topic | Key Files | Relevance to NPR |
|----|----|----|----|
| **HW1** | U-Nets & Diffusion | `2D_UNet.py`, `1D_UNet.py` | â­â­â­â­â­ High |
| **HW2** | VAE | `VAE.py`, `ELBO.py` | â­â­â­ Medium |
| **HW3** | GANs | `GAN.py`, `train.py` | â­â­â­â­ High |
| **HW4** | Diffusion Models | `train.py`, `forward_process.py` | â­â­â­â­â­ Very High |

---

## HW1: U-Nets & Attention Mechanisms â­â­â­â­â­

### **What You Implemented in HW1:**

#### 1. **Attention Mechanism** (`LinearAttention` and `Attention` classes)
```python
class LinearAttention(nn.Module):
    def __init__(self, in_channels, heads, dim_head):
        self.qkv = nn.Conv2d(in_channels, 3*hidden_dim, kernel_size=1)
        # ... attention computation

    def forward(self, x):
        q, k, v = self.qkv(x).chunk(3, dim=1)
        q = q.softmax(dim=2) * self.scale
        k = k.softmax(dim=3)
        context = torch.einsum('bhdn,bhen->bhde', k, v)
        out = torch.einsum('bhde,bhdn->bhen', context, q)
        return self.output(out)
```

**How We Adapted It for NPR:**
```python
# In networks/multiscale_npr.py: AttentionFusionModule
class AttentionFusionModule(nn.Module):
    def __init__(self, feature_dim=128, num_scales=3):
        self.attention_net = nn.Sequential(
            nn.Linear(feature_dim * num_scales, feature_dim // reduction),
            nn.ReLU(inplace=True),
            nn.Dropout(0.1),
            nn.Linear(feature_dim // reduction, num_scales),
            nn.Softmax(dim=1)  # â† Similar softmax attention!
        )

    def forward(self, features_list):
        all_features = torch.cat(features_list, dim=1)
        attention_weights = self.attention_net(all_features)  # Learn weights

        # Weighted fusion (similar to attention context)
        fused = torch.zeros_like(features_list[0])
        for i, features in enumerate(features_list):
            fused += attention_weights[:, i:i+1] * features

        return fused, attention_weights
```

**Key Adaptation:**
- **HW1**: Attention over spatial positions (Q, K, V)
- **NPR**: Attention over scales (weight 3 feature vectors)
- **Common Concept**: Softmax-based adaptive weighting

---

#### 2. **Residual Blocks** (`ResBlock` class)
```python
# From HW1
class ResBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        self.block1 = Block(in_channels, out_channels)
        self.block2 = Block(out_channels, out_channels)
        self.skip = nn.Conv2d(in_channels, out_channels, kernel_size=1)

    def forward(self, x):
        h = self.block1(x)
        return self.block2(h) + self.skip(x)  # â† Skip connection!
```

**How We Use It in NPR:**
```python
# networks/resnet.py (already in baseline) and our multiscale_npr.py
class BasicBlock(nn.Module):  # ResNet's basic residual block
    def forward(self, x):
        identity = x
        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)
        out = self.conv2(out)
        out = self.bn2(out)
        out += identity  # â† Same skip connection concept!
        out = self.relu(out)
        return out
```

**Key Adaptation:**
- **HW1**: ResBlock in U-Net encoder/decoder
- **NPR**: BasicBlock in ResNet branches
- **Common Concept**: Skip connections to help gradient flow

---

#### 3. **Multi-Scale Processing** (`UpBlock` and `DownBlock`)
```python
# From HW1
class DownBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        self.rearrange = Rearrange('b c (h p1) (w p2) -> b (c p1 p2) h w', p1=2, p2=2)
        self.conv = nn.Conv2d(in_channels * 4, out_channels, kernel_size=3, padding=1)

class UpBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        self.upsample = nn.Upsample(scale_factor=2)
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)
```

**How We Adapted It for NPR:**
```python
# In networks/multiscale_npr.py: NPRExtractor
class NPRExtractor(nn.Module):
    def __init__(self, scale_factor=0.5):
        self.scale_factor = scale_factor

    def interpolate(self, img, factor):
        downsampled = F.interpolate(img, scale_factor=factor, mode='nearest')
        upsampled = F.interpolate(downsampled, scale_factor=1/factor, mode='nearest')
        return upsampled  # â† Similar upsampling concept!

    def forward(self, x):
        reconstructed = self.interpolate(x, self.scale_factor)
        npr = x - reconstructed  # Extract multi-scale artifacts
        return npr
```

**Key Adaptation:**
- **HW1**: U-Net processes multiple scales sequentially (encoder â†’ decoder)
- **NPR**: We process multiple scales in parallel (3 branches)
- **Common Concept**: Multi-scale feature extraction

---

## HW2: VAE (Encoder-Decoder) â­â­â­

### **What You Implemented in HW2:**

#### 1. **Encoder (Downsampling with Conv2d)**
```python
class Encoder(nn.Module):
    def __init__(self, latent_dim=1):
        self.conv1 = nn.Conv2d(1, 32, 4, 2, 1)   # Downsample
        self.conv2 = nn.Conv2d(32, 64, 4, 2, 1)  # Downsample more

    def forward(self, x):
        h = F.relu(self.conv1(x))
        h = F.relu(self.conv2(h))
        h = h.view(h.size(0), -1)  # Flatten
        return mu, logvar
```

#### 2. **Decoder (Upsampling with ConvTranspose2d)**
```python
class Decoder(nn.Module):
    def __init__(self, latent_dim=1):
        self.deconv1 = nn.ConvTranspose2d(64, 32, 4, 2, 1)  # Upsample
        self.deconv2 = nn.ConvTranspose2d(32, 1, 4, 2, 1)   # Upsample more

    def forward(self, z):
        h = F.relu(self.fc(z))
        h = h.view(z.size(0), 64, 7, 7)
        h = F.relu(self.deconv1(h))  # â† Learned upsampling
        x_mu = self.deconv2(h)
        return x_mu
```

**How This Relates to NPR:**

**Understanding Upsampling Artifacts:**
- **HW2**: You learned that VAEs use `ConvTranspose2d` for upsampling
- **GANs** (like ProGAN, StyleGAN): Also use learned upsampling (transposed conv, bilinear)
- **NPR Insight**: Different upsampling methods (learned vs. nearest-neighbor) create different artifacts!

**NPR's Key Idea:**
```python
# NPR uses FIXED nearest-neighbor upsampling
x_reconstructed = F.interpolate(F.interpolate(x, 0.5), 2.0, mode='nearest')

# This is DIFFERENT from GAN's learned upsampling (ConvTranspose2d)
# The mismatch creates distinctive artifacts!
```

**Key Connection:**
- **HW2**: Taught you about ConvTranspose2d (learned upsampling)
- **NPR**: Exploits the difference between learned upsampling (GANs) and fixed upsampling (nearest-neighbor)
- **Your Contribution**: Multi-scale analysis tests if artifacts appear at different scales for different upsampling methods

---

## HW3: GANs (Binary Classification) â­â­â­â­

### **What You Implemented in HW3:**

#### 1. **Discriminator (Binary Real/Fake Classifier)**
```python
class Discriminator(nn.Module):
    def __init__(self):
        self.net = nn.Sequential(
            nn.Linear(2, 64),
            nn.LeakyReLU(),
            nn.Linear(64, 1),
            nn.Sigmoid()  # Binary classification!
        )

    def forward(self, x):
        return self.net(x)  # Returns probability [0,1]
```

#### 2. **Discriminator Training (Real vs Fake)**
```python
def train_discriminator(D, G, d_optimizer, batch_size, device):
    real_data = sample_real_data(batch_size)
    fake_data = G(torch.randn(batch_size, 2)).detach()

    d_real = D(real_data)
    d_fake = D(fake_data)

    # Binary classification loss: D(real) â†’ 1, D(fake) â†’ 0
    d_loss = -torch.log(d_real).mean() - torch.log(1 - d_fake).mean()

    d_optimizer.zero_grad()
    d_loss.backward()
    d_optimizer.step()
```

**How We Adapted It for NPR:**

**NPR is essentially a discriminator!**
```python
# NPR's classification head (in resnet.py and multiscale_npr.py)
self.fc = nn.Linear(feature_dim, 1)  # Binary classification

# Training with BCE Loss (similar to discriminator)
loss = nn.BCEWithLogitsLoss()(predictions, labels)
# labels: 0 = real, 1 = fake

# This is EXACTLY the same concept as GAN discriminator training!
```

**Key Adaptation:**
- **HW3**: Discriminator learns to distinguish real vs fake data
- **NPR**: CNN learns to distinguish real vs fake images based on NPR artifacts
- **Common Concept**: Binary classification with BCE loss

**Deep Connection:**
> "Your HW3 taught you how GANs create fake data and how discriminators detect it. Your NPR project is essentially a more sophisticated discriminator that detects GAN-generated images by analyzing their upsampling artifacts!"

---

## HW4: Diffusion Models â­â­â­â­â­

### **What You Implemented in HW4:**

#### 1. **Forward Diffusion Process**
```python
class DiffusionSampler(nn.Module):
    def q_sample(self, x0, t, noise=None):
        # Forward process: gradually add noise
        alpha_bar = extract(self.alpha_bar, t, x0.shape)
        xt = torch.sqrt(alpha_bar) * x0 + torch.sqrt(1 - alpha_bar) * noise
        return xt, noise
```

#### 2. **Reverse Denoising Process**
```python
    def p_sample(self, model, xt, t):
        # Reverse process: denoise step by step
        # Model predicts noise, we subtract it
        predicted_noise = model(xt, t)
        # ... compute x_{t-1} from x_t
        return sample
```

#### 3. **U-Net Architecture for Diffusion**
- U-Net with skip connections
- Time conditioning
- Multi-scale processing

**How This Relates to NPR:**

**Why Diffusion Models Are Hard to Detect:**

Diffusion models (DALL-E, Midjourney, Stable Diffusion) use:
1. **U-Net architecture** (you implemented this!)
2. **Iterative denoising** (gradual refinement)
3. **Learned upsampling** in the U-Net decoder

**The Challenge:**
```python
# Diffusion upsampling (from U-Net decoder)
- More sophisticated than simple ConvTranspose2d
- Multiple scales processed hierarchically
- Skip connections preserve high-freq details

# This makes artifacts MORE SUBTLE than GAN artifacts!
```

**Your Multi-Scale NPR Solution:**
> "Since diffusion models use U-Net with multi-scale processing, their artifacts may appear at different scales than GAN artifacts. Our multi-scale NPR with attention can adaptively detect these scale-dependent patterns!"

**Hypothesis from HW4 Knowledge:**
- **GANs** (HW3): Simple generator, artifacts at coarser scales
- **Diffusion** (HW4): U-Net with skip connections, artifacts at finer scales
- **Your Project**: Tests this hypothesis with multi-scale attention!

---

## Complete Mapping: Homework â†’ NPR Project

### **Architecture Components**

| Component | From Homework | Adapted To NPR | File Location |
|-----------|--------------|----------------|---------------|
| **Attention Mechanism** | HW1: `LinearAttention` | `AttentionFusionModule` | `networks/multiscale_npr.py:26` |
| **Residual Blocks** | HW1: `ResBlock` | `BasicBlock` | `networks/resnet.py:31` |
| **Multi-Scale** | HW1: `UpBlock`/`DownBlock` | `NPRExtractor` | `networks/multiscale_npr.py:59` |
| **Binary Classifier** | HW3: `Discriminator` | `classifier` layer | `networks/multiscale_npr.py:160` |
| **Conv2d Layers** | HW2: `Encoder` | `MultiScaleResNet` | `networks/multiscale_npr.py:88` |

---

### **Training Concepts**

| Concept | From Homework | Applied To NPR |
|---------|--------------|----------------|
| **BCE Loss** | HW3: Discriminator loss | Real/Fake classification |
| **Adam Optimizer** | HW2, HW3, HW4: Standard | Training multi-scale model |
| **Batch Normalization** | HW1: U-Net blocks | ResNet branches |
| **Skip Connections** | HW1: ResBlock | Residual learning |

---

### **Understanding Generative Models**

| HW | What You Learned | How It Helps NPR |
|----|------------------|------------------|
| **HW2** | VAE upsampling with ConvTranspose2d | Understand learned upsampling |
| **HW3** | GAN generator creates fake images | Know what you're detecting! |
| **HW4** | Diffusion U-Net multi-scale process | Hypothesis: diffusion = fine-scale artifacts |

---

## For Your Presentation Slides

### **Slide: "Building on Course Foundations"**

```
Homework Concepts Applied to Multi-Scale NPR
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

HW1: U-Nets & Attention
â”œâ”€ LinearAttention â†’ AttentionFusionModule
â”œâ”€ ResBlock â†’ BasicBlock (residual connections)
â””â”€ Multi-scale processing â†’ Parallel NPR extraction

HW2: VAE Encoder-Decoder
â”œâ”€ ConvTranspose2d â†’ Understanding upsampling
â””â”€ Conv2d â†’ Feature extraction branches

HW3: GANs
â”œâ”€ Discriminator â†’ Binary classifier (real/fake)
â””â”€ BCE Loss â†’ Training objective

HW4: Diffusion Models
â”œâ”€ U-Net multi-scale â†’ Hypothesis about fine-scale artifacts
â”œâ”€ Understanding diffusion upsampling â†’ Test set motivation
â””â”€ Iterative refinement â†’ Why diffusion is hard to detect
```

---

### **Slide: "Novel vs. Adapted"**

**From Homework** âœ…:
- Attention mechanism (softmax weighting)
- Residual blocks (skip connections)
- Binary classification (BCE loss)
- Understanding of GAN/Diffusion upsampling

**Novel Contribution** ðŸ†•:
- **Multi-scale NPR extraction** at 3 scales
- **Attention over scales** (not spatial attention)
- **Scale-specific hypotheses** (GANs vs Diffusion)
- **Interpretability focus** (which scales matter?)

---

### **Slide: "From Theory to Practice"**

**HW3 taught us**: GANs use learned upsampling (ConvTranspose2d)
**HW4 taught us**: Diffusion uses U-Net with multi-scale processing
**Insight**: Different upsampling â†’ different artifacts â†’ different scales!

**Our Hypothesis** (based on homework understanding):
```
GANs (HW3)        â†’ Coarse-scale artifacts (0.25Ã—, 0.5Ã—)
Diffusion (HW4)   â†’ Fine-scale artifacts (0.5Ã—, 0.75Ã—)
Attention (HW1)   â†’ Learn which scales matter!
```

---

## Talking Points for Q&A

**Q: "Which homework concepts did you use?"**
> "We adapted attention mechanisms from HW1, binary classification from HW3, and applied our understanding of GAN/Diffusion architectures from HW2-4. Specifically, the AttentionFusionModule uses softmax attention similar to HW1's LinearAttention, but applied over scales rather than spatial positions."

**Q: "How did your homework help with this project?"**
> "HW4 on diffusion models was crucial. We learned that diffusion models use U-Nets with multi-scale processing, which led to our hypothesis that they leave artifacts at different scales than GANs. HW3 taught us binary classification for real/fake detection, which is exactly what NPR does."

**Q: "What's new vs. what's from class?"**
> "The attention mechanism and residual blocks are standard techniques we learned in homework. What's novel is applying them to multi-scale NPR extractionâ€”using attention to learn which scales matter for different generators. This specific application to deepfake detection is our contribution."

---

## Code Citations for Your Report

When writing your report, cite homework like this:

```
The attention-based fusion module (networks/multiscale_npr.py:26-56)
adapts the attention mechanism from HW1 (2D_UNet.py:65-106), using
softmax-weighted combination of features. However, instead of spatial
attention over image positions, we apply attention over scale-specific
features, learning which NPR scales are most discriminative.

The multi-scale architecture draws inspiration from HW1's U-Net
encoder-decoder structure and HW4's understanding of diffusion models'
multi-scale processing. Our hypothesis that different generators leave
artifacts at different scales stems from analyzing the upsampling
strategies in HW2 (VAE), HW3 (GAN), and HW4 (Diffusion).
```

---

## Summary: Complete Homework Integration

âœ… **HW1**: Attention mechanism, residual blocks, multi-scale concepts
âœ… **HW2**: Understanding upsampling (ConvTranspose2d), encoder-decoder
âœ… **HW3**: Binary classification, discriminator training, GANs
âœ… **HW4**: Diffusion models, U-Net, why diffusion is hard to detect

ðŸ†• **Your Contribution**: Combining these concepts into multi-scale NPR with adaptive attention fusion for interpretable deepfake detection!

---

**Total Code Reuse**: ~200 lines of concepts (attention, residual, BCE loss)
**New Code Written**: ~950 lines (multi-scale NPR, fusion, visualization)
**Novel Application**: Multi-scale artifact analysis with learned fusion

This shows you **built on solid foundations** from coursework while making **significant original contributions**! ðŸŽ¯
